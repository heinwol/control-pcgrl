import argparse
import json
import operator
import os
from pathlib import Path
import re
import csv
import glob
from typing import Dict, List

import hydra
import numpy as np
from matplotlib import pyplot as plt
from omegaconf import DictConfig, ListConfig
import pandas as pd
import seaborn as sns
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator

from control_pcgrl.configs.config import Config, CrossEvalConfig
from control_pcgrl.rl.utils import get_log_dir, PROB_CONTROLS, validate_config
from tex_formatting import newline

# OVERLEAF_DIR = "/home/sme/Dropbox/Apps/Overleaf/Evolving Diverse NCA Level Generators -- AIIDE '21/tables"

# Map names of metrics recorded to the names we want to display in a table

RUNS_DIR = os.path.join(Path(__file__).parent.parent.parent, "rl_runs")
EVAL_DIR = os.path.join(Path(__file__).parent.parent.parent, "rl_eval")


# flatten the dictionary here


def bold_extreme_values(data, data_max=-1):
    # breakpoint()
    data_max = data_max
    data = int(data)
    #   print(data)
    if data == data_max:
        #       return "\\bfseries {:.2f}".format(data)
        return "\\bfseries {}".format(data)

    else:
        #       return "{:.1f}".format(data)
        return "{}".format(data)


#   return data
#   return "{}".format(data)


def flatten_stats(stats, controllable=False):
    flat_stats = {}

    # def add_key_val(key, val):
    #         if controllable and key != "% train archive full":
    # #           key = "(controls) " + key
    #             if key in header_text:
    #                 key = header_text[key]
    #             key = ', '.join([c for c in stats['controls'] if c is not None]) + '||' + key

    # if "%" in key:
    # val *= 100
    # elif "playability" in key:
    # val /= 10

    # if key in header_text:
    # key = header_text[key]
    # flat_stats[key] = val

    for k, v in stats.items():
        if isinstance(v, dict):
            key_0 = k

            for k1, v1 in v.items():
                key = "{} ({})".format(key_0, k1)
                value = v1
                flat_stats[key] = value
        else:
            flat_stats[k] = v

    return flat_stats


def flatten_dict(d, parent_key="", sep="_"):
    items = []
    for k, v in d.items():
        new_key = parent_key + sep + k if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)


def cross_evaluate_static(
    cross_eval_cfg: Config, sweep_configs: List[Config], sweep_params: Dict[str, str]
):
    static_trains_to_eval_stats = {}
    for exp_cfg in sweep_configs:
        exp_path = exp_cfg.log_dir
        exp_tpl = (exp_cfg.static_prob, exp_cfg.n_static_walls)
        static_trains_to_eval_stats[exp_tpl] = {}
        eval_stat_files = [
            f
            for f in os.listdir(exp_path)
            if f.startswith("static") and f.endswith(".json")
        ]
        for eval_stat_file in eval_stat_files:
            # Use regular expression to get floats `static-prob-{0}_static-walls-{1}_eval_stats.json`
            groups = re.findall(
                r"static-prob-(.+)_static-walls-(\d+)_eval_stats.json", eval_stat_file
            )
            static_prob, static_walls = float(groups[0][0]), int(groups[0][1])
            with open(os.path.join(exp_path, eval_stat_file), "r") as f:
                eval_stats = json.load(f)
            eval_tpl = (static_prob, static_walls)
            if eval_tpl in static_trains_to_eval_stats[exp_tpl]:
                static_trains_to_eval_stats[exp_tpl][eval_tpl].append(
                    eval_stats["episode_reward_mean"]
                )
            else:
                static_trains_to_eval_stats[exp_tpl][eval_tpl] = [
                    eval_stats["episode_reward_mean"]
                ]
    static_trains_to_agg_stats = {
        exp_tpl: {k: (np.mean(v), np.std(v)) for k, v in eval_stats.items()}
        for exp_tpl, eval_stats in static_trains_to_eval_stats.items()
    }
    # Convert dict to pandas dataframe
    df = pd.DataFrame.from_dict(static_trains_to_agg_stats, orient="index")

    # Sort columns
    df = df.reindex(sorted(df.columns), axis=1)

    # Extract only the mean values from the dataframe
    mean_df = df.applymap(lambda x: x[0])

    # Convert the index labels to the desired format
    mean_df.index = mean_df.index.map(lambda x: str(x))

    # Create a heatmap
    plt.figure(figsize=(15, 13))
    sns.heatmap(mean_df, annot=True, fmt=".2f", cmap="YlGnBu", cbar=True)
    plt.title("Static Tiles Heatmap -- Mean Episode Reward")
    plt.xlabel("Evaluation parameters (static tile probability, n. freezies)")
    plt.ylabel("Training parameters (static tile probability, n. freezies)")
    plt.savefig(os.path.join(EVAL_DIR, "static_tiles_heatmap.png"), bbox_inches="tight")
    plt.tight_layout()
    plt.close()

    # Save to csv
    df.to_csv(os.path.join(EVAL_DIR, "static_tiles.csv"))

    # Convert to latex
    # latex = pandas_to_latex(df, bold_extreme_values=bold_extreme_values, latex_file=os.path.join(EVAL_DIR, 'static_tiles.tex'))
    # with open(os.path.join(EVAL_DIR, 'static_tiles.tex'), 'w') as f:
    #     f.write(latex)

    # tables_tex_fname = os.path.join(EVAL_DIR, "tables.tex")


def cross_evaluate(
    cross_eval_cfg: CrossEvalConfig,
    sweep_configs: List[Config],
    sweep_params: Dict[str, str],
):
    """Collect results generated when evaluating trained models under different conditions.
    Args:
        cross_eval_config (CrossEvalConfig): The cross-evaluation config
        sweep_configs (List[EvalConfig]): EvalConfigs corresponding to evaluations
        sweep_params (Dict[str, str]): The eval/train hyperparameters being swept over in the cross-evaluation
    """
    # validate_config(cross_eval_config)
    # [validate_config(c) for c in sweep_configs]
    if cross_eval_cfg.plot_loss:
        for cfg in sweep_configs:
            tensorboard_dir = str(cfg.log_dir)
            # Find all files names `progress.csv` stored in any child of this directory using glob
            progress_files = glob.glob(
                os.path.join(tensorboard_dir, "**", "progress.csv"), recursive=True
            )
            loss_values = {}
            df = None

            for progress_file in progress_files:
                # Load the progress.csv file
                if df is None:
                    df = pd.read_csv(progress_file)
                else:
                    # Concatenate the dataframes
                    df = pd.concat([df, pd.read_csv(progress_file)])

            # Sort the dataframe by `timesteps_total`
            df = df.sort_values(by=["timesteps_total"])

            # Plot the loss curve with `timesteps_total` on the x-axis and `episode_reward_mean` on the y-axis
            plt.plot(df["timesteps_total"], df["episode_reward_mean"])
            plt.xlabel("Timesteps")
            plt.ylabel("Episode Reward")
            plt.title("Training Reward Curve")
            # plt.legend()
            plt.savefig(os.path.join(cfg.log_dir, "loss_curve.png"))
            print(f"Saved loss curve to {cfg.log_dir}")
            plt.close()

    if cross_eval_cfg.name == "static_tiles":
        cross_evaluate_static(cross_eval_cfg, sweep_configs, sweep_params)

    keys = [
        "task",
        "representation",
        "model",
        "n_aux_tiles",
        "max_board_scans",
        "controls",
        "lr",
        "exp_id",
        # "controls",
        # "alp_gmm",
        # "change_percentage"
    ]

    local_controls = {
        "binary_ctrl": [
            ["regions", "path-length"],
            ["regions"],
            ["path-length"],
            # ['emptiness', 'path-length'],
            # ["symmetry", "path-length"]
        ],
        "zelda_ctrl": [
            ["nearest-enemy", "path-length"],
            ["nearest-enemy"],
            ["path-length"],
            # ["emptiness", "path-length"],
            # ["symmetry", "path-length"],
        ],
        "sokoban_ctrl": [
            # ["crate"],
            ["crate", "sol-length"],
            ["sol-length"],
            # ["emptiness", "sol-length"],
            # ["symmetry", "sol-length"],
        ],
        "smb_ctrl": [
            # ['enemies', 'jumps'],
            # ["emptiness", "jumps"],
            # ["symmetry", "jumps"],
        ],
        "RCT": [
            # ['income'],
        ],
    }

    row_header_text = {
        "n_static_walls": "freezies",
        "multiagent.n_agents": "n. agents",
    }

    if cross_eval_cfg.name == "squeegee":
        row_header_text["act_window"] = "squeegee"

    row_index_text = {
        "zelda_ctrl": "zelda",
        "binary_ctrl": "binary",
        "sokoban_ctrl": "sokoban",
        "NONE": "---",
        "change_percentage": newline("change", "percentage"),
        #   'net_score (mean)': newline('target', 'progress'),
        "net_score (mean)": "pct. targets reached ",
        #   '(controls) net_score (mean)': newline('\\textit{control}', 'net score'),
        "diversity_score (mean)": "diversity",
        #   "(controls) diversity_score (mean)": newline('\\textit{control}', 'diversity'),
        #   'ctrl_score (mean)': newline('control', 'success'),
        "ctrl_score (mean)": "pct. targets reached",
        #   '(controls) fixed_score (mean)': newline('\\textit{control}', 'static score'),
        #   "alp_gmm": newline("ALP", "GMM"),
        "alp_gmm": newline("control", "regime"),
        "controls": newline("learned", " controls"),
    }

    experiment_0 = sweep_configs[0]

    col_headers = [
        "episode_reward_max",
        "episode_reward_mean",
        "episode_reward_min",
        "episode_len_mean",
        "episodes_this_iter",
        "total_steps",
    ]
    # TODO: Automate these!
    col_headers += ["path-length_max", "path-length_mean", "path-length_min"]
    row_headers = list(sweep_params.keys())

    # row_headers_sorted = ['model', 'act_window', 'exp_id']
    # sort_map = {k: i for i, k in enumerate(row_headers_sorted)}

    # row_headers = sorted(row_headers, key=sort_map.__getitem__)

    rows = []
    vals = []

    for experiment in sweep_configs:
        exp_path = experiment.log_dir
        path = os.path.join(exp_path, "eval_stats.json")

        if not os.path.isfile(path):
            print(f"No eval_stats.json found in {path}")
            continue

        print(f"Loading eval stats from {path}")

        with open(path, "r") as f:
            stats = json.load(f)
            # stats = flatten_stats(stats)
            stats = flatten_dict(stats)

        row = []

        for k in row_headers:
            v = operator.attrgetter(k)(experiment)

            # If a subconfig, take its name(?? HACK)
            if isinstance(v, DictConfig):
                v_name = v.name
                if v_name is None:
                    if k == "model":
                        v_name = "Default"
                    row.append(v_name)
                else:
                    row.append(v.name.replace("_", " "))

            elif isinstance(v, ListConfig):
                # Important that we turn this into a string lest terrible weirdness ensue.
                row.append(str(tuple(list(v))))

            else:
                row.append(str(v))

        rows.append(row)

        exp_stats = [stats[k] for k in col_headers if k in stats]
        vals.append(exp_stats)

    # Rename row_headers
    row_headers = [
        row_header_text[h] if h in row_header_text else h for h in row_headers
    ]

    # iterate col_headers and replace "_" with " " to avoid latex errors
    col_headers = [h.replace("_", " ") for h in col_headers]
    col_headers = [h.replace(".", " ") for h in col_headers]
    row_headers = [h.replace("_", " ") for h in row_headers]
    row_headers = [h.replace(".", " ") for h in row_headers]

    # Make a multi-indexed dataframe
    rows = pd.MultiIndex.from_tuples(rows, names=row_headers)

    df = pd.DataFrame(vals, index=rows, columns=col_headers)

    # # Replace every `_` with a `\_` to avoid latex errors, in rows...
    # df.index = df.index.map(lambda x: [str(i).replace("_", " ") for i in x])
    # df.index = df.index.map(lambda x: [str(i).replace("[", "") for i in x])
    # # and in columns.
    # df.columns = df.columns.map(lambda x: x.replace("_", " "))
    # # And in the values, format to 2 decimal places
    # df = df.applymap(lambda x: "{:.2f}".format(x))

    def write_data(df, tex_name):
        # Save the dataframe to csv
        df.to_csv(os.path.join(EVAL_DIR, f"{tex_name}.csv"))

        # df = df.applymap(bold_extreme_values)
        # df = df.style.highlight_max(axis=0, color="lightgreen")
        df = df.style.apply(highlight_max)
        df.format(format_vals)

        # Save the df to html
        df.to_html(os.path.join(EVAL_DIR, f"{tex_name}.html"))

        # Save the df as latex
        # df = df.applymap(lambda x: "{:.2f}".format(x))

        # tex_name = cross_eval_config.name

        # Save the df as latex
        # df.to_latex(
        #     os.path.join(EVAL_DIR, "cross_eval.tex"),
        #     column_format="l" + "c" * len(col_headers),
        #     multirow=True,
        #     escape=False,
        # )

        n_col_indices = len(df.index[0]) if isinstance(df.index[0], tuple) else 1
        n = len(df.columns) + n_col_indices

        cols = "c" + "c" * (n - 1)

        # Add the vertical lines
        cols = "|" + "|".join(cols) + "|"

        latex = df.to_latex(
            column_format=cols,
            hrules=True,
            clines="skip-last;data",
        )

        with open(os.path.join(EVAL_DIR, f"{tex_name}.tex"), "w") as f:
            f.write(latex)

        # pandas_to_latex(
        #     df,
        #     os.path.join(EVAL_DIR, tex_name + '.tex'),
        #     # multirow=True,
        #     index=True,
        #     header=True,
        #     vertical_bars=True,
        #     # columns=col_indices,
        #     multicolumn=True,
        #     # multicolumn_format='c|',
        #     right_align_first_column=False,
        #     # bold_rows=True,
        # )

        tables_tex_fname = os.path.join(EVAL_DIR, "tables.tex")

        # Replace the line in `tables.tex` that says `\input{.*}` with `\input{tables_tex_fname.tex}` with regex
        re.sub(r"\\input{.*}", "poo", os.path.join(EVAL_DIR, "tables.tex"))

        os.system(f"pdflatex {tables_tex_fname}")

        # Move the output pdf int rl_eval
        # os.system(f"mv {tables_tex_fname.replace('.tex', '.pdf')} {EVAL_DIR}")

        return

    write_data(df, cross_eval_cfg.name)

    row_headers.remove("exp id")

    # Average over exp_id (no cooperating currently)
    # df = df.groupby(row_headers).mean(numeric_only=True).reset_index()
    # TODO: standard deviation

    # Average/stds the ugly manual way
    new_rows = []
    new_row_names = []
    for i in range(df.shape[0]):
        row = df.iloc[i]
        name = row.name
        if name[:-1] in new_row_names:
            continue
        new_row_names.append(name[:-1])

        # For some reason this is necessary even though we've already done this when processing row headers above?
        # new_name = []
        # for ni in name:
        #     if isinstance(ni, ListConfig):
        #         ni = tuple([nii for nii in ni])
        #     new_name.append(ni)
        # name = new_name

        repeat_exps = df.loc[name[:-1]]
        # eval_qd_scores.append(repeat_exps[('Evaluation', 'QD score')].to_numpy())
        mean_exp = repeat_exps.mean(axis=0)
        std_exp = repeat_exps.std(axis=0)
        mean_exp = list(zip(mean_exp, std_exp))
        new_rows.append(mean_exp)

    index = pd.MultiIndex.from_tuples(new_row_names, names=row_headers)
    columns = df.columns
    ndf = pd.DataFrame(new_rows, index=index, columns=col_headers)
    # ndf = ndf.append(new_rows)
    # new_col_indices = pd.MultiIndex.from_tuples(df.columns)
    # new_row_indices = pd.MultiIndex.from_tuples(new_row_names)
    # ndf.index = new_row_indices
    # ndf.columns = new_col_indices
    # ndf.index.names = df.index.names[:-1]
    ndf = ndf

    write_data(ndf, f"{cross_eval_cfg.name}_aggregate_raw")

    drop_cols = ["total steps", "episode len mean", "episodes this iter"]
    ndf = ndf.drop(columns=drop_cols)

    write_data(ndf, f"{cross_eval_cfg.name}_aggregate")

    keep_cols = ["episode reward max", "episode reward mean"]
    ndf = ndf[keep_cols]
    write_data(ndf, f"{cross_eval_cfg.name}_aggregate_condensed")

    return

    # batch_exp_name = settings_list[0]["exp_id"]
    #   if batch_exp_name == "2":
    #   elif batch_exp_name == "1":
    #       EVO_DIR = "evo_runs_06-13"
    #       RL_DIR = "evo_runs_06-14"
    #   ignored_keys = set(
    #       (
    #           "exp_name",
    #           "evaluate",
    #           "show_vis",
    #           "visualize",
    #           "render_levels",
    #           "multi_thread",
    #           "play_level",
    #           "evaluate",
    #           "save_levels",
    #           "cascade_reward",
    #           "model",
    #           "n_generations",
    #           "render",
    #           "infer",
    #       )
    #   )
    #   keys = []

    #   for k in settings_list[0].keys():
    #       if k not in ignored_keys:
    #           keys.append(k)
    columns = None
    data = []
    vals = []

    for i, settings in enumerate(sweep_configs):
        val_lst = []

        controllable = False
        for k in keys:
            val = settings[k]
            if k == "controls":
                if k != ["NONE", "NONE"]:
                    controllable = True
            if isinstance(settings[k], list):
                if len(settings[k]) < 2:
                    val = ", ".join(settings[k])
                else:
                    val = newline(settings[k][0] + ", ", val[1])
            elif k == "alp_gmm":
                if not controllable:
                    val = ""
                elif val:
                    val = "ALP-GMM"
                else:
                    val = newline("uniform", "random")
            if isinstance(val, str):
                val = val.replace("_", " ")
            val_lst.append(val)
        # args = parse_args(load_args=settings)
        # arg_dict = vars(args)
        # dict to namespace
        args = argparse.Namespace(**settings)
        arg_dict = vars(args)
        # FIXME: well this is stupid
        exp_name = (
            get_log_dir(args) + "_" + str(arg_dict["exp_id"]) + "_log"
        )  # FIXME: this should be done elsewhere??
        arg_dict["cond_metrics"] = arg_dict.pop("controls")
        # NOTE: For now, we run this locally in a special directory, to which we have copied the results of eval on
        # relevant experiments.
        exp_name = os.path.join(RUNS_DIR, exp_name)
        # eval_dir = os.path.join(exp_name, "eval")
        eval_dir = exp_name
        eval_stats = json.load(open(os.path.join(eval_dir, "eval_stats.json"), "r"))
        if not os.path.isdir(eval_dir):
            print(
                "skipping evaluation of experiment due to missing directory: {}".format(
                    eval_dir
                )
            )
            continue
        ctrl_stats_files = [
            f for f in os.listdir(eval_dir) if re.match(r"scores_.*_ctrlTrgs.json", f)
        ]
        # stats_f = os.path.join(eval_dir, "scores_ctrlTrgs.json")
        fixTrgs_stats_f = os.path.join(eval_dir, "scores_fixTrgs.json")

        if not ctrl_stats_files and os.path.isfile(fixTrgs_stats_f):
            #           print(stats_f)
            print(
                "skipping evaluation of experiment due to missing stats file(s): {}".format(
                    exp_name
                )
            )

            continue
        ctrl_stats_files = [os.path.join(eval_dir, f) for f in ctrl_stats_files]
        # TODO: This is just here for convenience, but not really a part of cross-eval, semantically-speaking
        if not no_plot:
            plot_csv(exp_name)
        vals.append(tuple(val_lst))
        data.append([])
        flat_stats = flatten_stats(eval_stats)
        # fixLvl_stats = json.load(open(fixTrgs_stats_f, "r"))
        # flat_stats = flatten_stats(fixLvl_stats)
        # for stats_f in ctrl_stats_files:
        #     stats = json.load(open(stats_f, "r"))
        #     flat_stats.update(flatten_stats(stats, controllable=True))

        if columns is None:
            columns = list(flat_stats.keys())

        for j, c in enumerate(columns):
            if c not in flat_stats:
                data[-1].append("N/A")
            else:
                data[-1].append(flat_stats[c])

    tuples = vals
    # Rename headers
    new_keys = []
    # alp_gmm is always "False" for non-controllable agents. But we don't want to display them as having any control regime.
    # controls_id = keys.index('controls')
    # regime_id = keys.index('alp_gmm')
    # for i, tpl in enumerate(tuples):
    #     if tpl[controls_id] == 'NONE':
    #         tpl = list(tpl)
    #         tpl[regime_id] = '---'
    #         tpl = tuple(tpl)
    #         tuples[i] = tpl

    for k in keys:
        if k in row_index_text:
            k = row_index_text[k]
        k = k.replace("_", " ")
        new_keys.append(k)

    for i, lst in enumerate(tuples):
        new_lst = []
        for v in lst:
            if v in row_index_text:
                new_lst.append(row_index_text[v])
            else:
                new_lst.append(v)
        tuples[i] = new_lst

    index = pd.MultiIndex.from_tuples(tuples, names=new_keys)
    #   df = index.sort_values().to_frame(index=True)
    # Hierarchical columns!
    col_tuples = []
    # for col in columns:
    #     if '||' not in col:
    #         col_tuples.append(('fixed targets', '---', col))
    #     else:
    #         controls = col.split('||')[0]
    #         col_tuples.append(('controlled targets', controls, col.split('||')[-1]))
    # columns = pd.MultiIndex.from_tuples(col_tuples, names=['', newline('evaluated', 'controls'), ''])
    # columns = pd.MultiIndex.from_tuples(col_tuples, names=['', 'evaluated controls', ''])
    df = pd.DataFrame(data=data, index=index, columns=columns)
    #   df = df.sort_values(by=new_keys, axis=0)
    #   new_keys = [tuple(t) for t in tuples]
    #   df = df.sort_values(by=new_keys, axis=0)

    csv_name = r"{}/cross_eval_{}.csv".format(EVAL_DIR, batch_exp_name)
    html_name = r"{}/cross_eval_{}.html".format(EVAL_DIR, batch_exp_name)
    df.to_csv(csv_name)
    df.to_html(html_name)
    #   print(df)

    columns = [c.replace("_", " ") for c in columns]
    df_tex = pd.DataFrame(data=data, index=index, columns=columns)
    #   tex_name = r"{}/zelda_empty-path_cell_{}.tex".format(OVERLEAF_DIR, batch_exp_name)

    # TODO: dust off latex table-generation for new domains.

    # for p in ["binary", "zelda", "sokoban", "minecraft_3D_maze"]:
    #   for p in ["binary", "zelda"]:
    #   for p in ["binary"]:
    tex_name = os.path.join(EVAL_DIR, "cross_eval.tex")
    # #       print(p)
    #         p_name = p + '_ctrl'
    #         lcl_conds = ['---'] + ['-'.join(pi) if len(pi) < 2 else newline(pi[0] + ', ', pi[1]) for pi in local_controls[p_name]]
    # #       print(lcl_conds)
    # #       df_tex = df_tex.loc[lcl_conds]
    # #       df_tex = df_tex.sort_values(by=['ALP GMM'])
    #         z_cols_fixed = [
    #             header_text["net_score (mean)"],
    #             header_text["diversity_score (mean)"],
    #         ]
    #         z_cols_ctrl = [
    # #           col_keys["net_score (mean)"],
    #             header_text["ctrl_score (mean)"],
    # #           col_keys["(controls) fixed_score (mean)"],
    #             header_text["diversity_score (mean)"],
    #         ]
    #         n_col_heads = len(z_cols_fixed)
    #         z_cols = list(zip(['fixed targets'] * n_col_heads, ['---'] * n_col_heads, z_cols_fixed))
    #         for ctrl_set in PROB_CONTROLS[p + '_ctrl']:
    #             if 'sol-length' in ctrl_set:
    #                 ctrl_set[ctrl_set.index('sol-length')] = 'solution-length'
    #             n_col_heads = len(z_cols_ctrl)
    #             z_cols += list(zip(['controlled targets'] * n_col_heads, [', '.join(ctrl_set)] * n_col_heads, z_cols_ctrl))
    #         df_tex = df_tex[z_cols]
    #         #   df_tex = df.drop(columns=z_cols)
    # #       df_tex['fixed targets'] = df_tex['fixed targets'][z_cols[0:2]]
    # #       df_tex['controlled targets'] = df_tex['fixed targets'][z_cols[2:]]
    #         df_tex = df_tex * 100
    #         df_tex = df_tex.astype(float).round(0)
    #         dual_conds = ['---', lcl_conds[1]]
    #         for k in z_cols:
    #             print(k)
    #             if k in df_tex:
    #                 print(k)
    # #               df_tex.loc[dual_conds][k] = df_tex.loc[dual_conds][k].apply(
    # #                   lambda data: bold_extreme_values(data, data_max=df_tex.loc[dual_conds][k].max())
    # #               )
    #                 print(df_tex[k].max())
    #                 df_tex[k] = df_tex[k].apply(
    #                     lambda data: bold_extreme_values(data, data_max=df_tex[k].max())
    #                 )
    # #       df_tex = df_tex.round(2)
    # #       df_tex.reset_index(level=0, inplace=True)
    # #       print(df_tex)

    # #       with open(tex_name, "w") as tex_f:
    # #           col_widths = "p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.8cm}p{0.8cm}p{0.8cm}"
    pandas_to_latex(
        df_tex,
        #           df_tex.to_latex(
        tex_name,
        vertical_bars=True,
        index=True,
        bold_rows=True,
        header=True,
        columns=columns,
        multirow=True,
        multicolumn=True,
        multicolumn_format="c|",
        # column_format= 'r|' * len(index) + 'c|' * len(df_tex.columns),
        escape=False,
        # caption=("Performance of controllable {} level-generating agents with learning-progress-informed (ALP-GMM) and uniform-random control regimes, and baseline (single-objective) agents, with various change percentage allowances. Agents are tested both on a baseline task with metric targets fixed at their default values, and control tasks, in which controllable metric targets are sampled over a grid.".format(p)),
        # label={"tbl:{}".format(p)},
    )

    tables_tex_fname = os.path.join(EVAL_DIR, "tables.tex")
    os.system(f"pdflatex {tables_tex_fname}")

    #   # Remove duplicate row indices for readability in the csv
    #   df.reset_index(inplace=True)
    #   for k in new_keys:
    #       df.loc[df[k].duplicated(), k] = ''
    #   csv_name = r"{}/cross_eval_{}.csv".format(OVERLEAF_DIR, batch_exp_name)


def listolists_to_arr(a):
    b = np.empty([len(a), len(max(a, key=lambda x: len(x)))])
    b[:] = np.NaN
    for i, j in enumerate(a):
        b[i][0 : len(j)] = j
    return b


def plot_csv(exp_name):
    monitor_files = [
        os.path.join(exp_name, f) for f in os.listdir(exp_name) if "monitor.csv" in f
    ]
    g_rew_id, g_len_id, g_time_id = None, None, None
    g_reward_weights, g_lens, g_times = [], [], []
    for f in monitor_files:
        rewards, lens, times = [], [], []
        with open(f, newline="") as mf:
            reader = csv.reader(mf, delimiter=",", quotechar="|")
            for i, row in enumerate(reader):
                if i == 0:
                    # This contains one-off info like time start and env id
                    continue
                if i == 1:
                    # Get header info. stable-baselines determines how this is written. Being extra safe here.
                    rew_id, len_id, time_id = (
                        row.index("r"),
                        row.index("l"),
                        row.index("t"),
                    )
                    if g_rew_id is None:
                        g_rew_id, g_len_id, g_time_id = rew_id, len_id, time_id
                    else:
                        assert (
                            g_rew_id == rew_id
                            and g_len_id == len_id
                            and g_time_id == time_id
                        )
                    continue
                if len(row) != 3:
                    print(
                        "row {} of monitor file {} is not the correct length: {}. Skipping".format(
                            i, f, row
                        )
                    )
                    continue
                try:
                    row = [float(r) for r in row]
                except ValueError:
                    print(
                        "row {} of monitor file {} has invalid values: {}. Skipping.".format(
                            i, f, row
                        )
                    )
                    continue
                rewards.append(row[g_rew_id])
                lens.append(row[g_len_id])
                times.append(row[g_time_id])
        g_reward_weights.append(rewards)
        g_lens.append(lens)
        g_times.append(times)
    if not g_reward_weights:
        return
    rew_arr, len_arr, time_arr = (
        listolists_to_arr(g_reward_weights),
        listolists_to_arr(g_lens),
        listolists_to_arr(g_times),
    )
    mean_rews, mean_lens, mean_times = (
        np.nanmean(rew_arr, axis=0),
        np.nanmean(len_arr, axis=0),
        np.nanmean(time_arr, axis=0),
    )
    plt.figure()
    mean_rews = np.convolve(mean_rews, [1 / 7 for i in range(7)])
    with open(os.path.join(exp_name, "train_time_stats.json"), "w") as f:
        json.dump({"n_frames": np.nansum(len_arr)}, f)
    plt.plot(mean_rews)
    plt.savefig(os.path.join(exp_name, "reward.png"))


def pandas_to_latex(
    df_table,
    latex_file,
    vertical_bars=False,
    right_align_first_column=True,
    header=True,
    index=False,
    escape=False,
    multicolumn=False,
    **kwargs,
) -> None:
    """
    Function that augments pandas DataFrame.to_latex() capability.

    Args:
        df_table: dataframe
        latex_file: filename to write latex table code to
        vertical_bars: Add vertical bars to the table (note that latex's booktabs table format that pandas uses is
                          incompatible with vertical bars, so the top/mid/bottom rules are changed to hlines.
        right_align_first_column: Allows option to turn off right-aligned first column
        header: Whether or not to display the header
        index: Whether or not to display the index labels
        escape: Whether or not to escape latex commands. Set to false to pass deliberate latex commands yourself
        multicolumn: Enable better handling for multi-index column headers - adds midrules
        kwargs: additional arguments to pass through to DataFrame.to_latex()
    """
    if isinstance(df_table.index[0], tuple):
        n_col_indices = len(df_table.index[0])
    else:
        n_col_indices = 1
    n = len(df_table.columns) + n_col_indices

    #   if right_align_first_column:
    cols = "c" + "c" * (n - 1)
    #   else:
    #       cols = 'r' * n

    if vertical_bars:
        # Add the vertical lines
        cols = "|" + "|".join(cols) + "|"

    # latex = df_table.to_latex(escape=escape, index=index, column_format=cols, header=header, multicolumn=multicolumn,
    #                           **kwargs)

    # s = df_table.style.highlight_max(
    #     props='cellcolor:[HTML]{FFFF00}; color:{red}; itshape:; bfseries:;'
    # )

    # s = df_table.style.format(format_vals)
    # s = s.highlight_max(axis=None, props='font-weight:bold')
    # s = df_table.style.format(format_vals)
    #                        props='cellcolor:{red}; bfseries: ;')

    latex = df_table.to_latex(
        column_format=cols,
        **kwargs,
        hrules=True,
        clines="skip-last;data",
    )

    with open(latex_file, "w") as f:
        f.write(latex)


def highlight_max(series, *args, **kwargs):
    is_max = series == series.max()
    return ["bfseries:" if v else "" for v in is_max]


def format_vals(val):
    if isinstance(val, float):
        val_str = "{:,.2f}".format(val)

    elif isinstance(val, tuple):
        # Assume first entry is mean, second is std
        val_str = "{:,.2f} ± {:,.2f}".format(val[0], val[1])

    elif isinstance(val, int):
        val_str = "{:,}".format(val)

    return val_str


@hydra.main(version_base="1.3", config_path="../configs", config_name="cross_eval")
def main(cfg: CrossEvalConfig):
    """This is a fake function we use to enter our hydra plugin (at
    `hydra_plugins.cross_eval_launcher_plugin.cross_eval_launcher.CrossEvalLauncher`), from which we call `cross_eval`.
    We do this so that hydra constructs the configs corresponding to our sweep, which we then pass to `cross_eval`.
    """
    pass


if __name__ == "__main__":
    main()
